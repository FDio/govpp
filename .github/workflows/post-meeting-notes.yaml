name: Post FD.io meeting notes

on:
  schedule:
    - cron: "23 3 * * *"
  workflow_dispatch:
    inputs:
      days_back:
        description: "Days to look back for meeting notes"
        required: false
        default: "30"

permissions:
  contents: read
  discussions: write

jobs:
  post-meeting-notes:
    runs-on: ubuntu-latest
    steps:
      - name: Post meeting notes to discussion
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          DAYS_BACK: ${{ github.event.inputs.days_back }}
        run: |
          python - <<'PY'
          import datetime
          import json
          import os
          import re
          import sys
          import urllib.error
          import urllib.parse
          import urllib.request
          from html.parser import HTMLParser

          BASE_URL = "https://ircbot.wl.linuxfoundation.org/meetings/fdio-meeting/"
          OWNER = "FDio"
          REPO = "govpp"
          DISCUSSION_NUMBER = 46

          print(f"Starting meeting notes sync from {BASE_URL}")
          print(f"Target discussion: {OWNER}/{REPO}#{DISCUSSION_NUMBER}")

          class LinkParser(HTMLParser):
              def __init__(self):
                  super().__init__()
                  self.links = []

              def handle_starttag(self, tag, attrs):
                  if tag != "a":
                      return
                  for key, value in attrs:
                      if key == "href" and value:
                          self.links.append(value)

          def fetch_url(url):
              print(f"Fetching URL: {url}")
              try:
                  with urllib.request.urlopen(url) as response:
                      payload = response.read().decode("utf-8", errors="replace")
                      print(f"Fetched {len(payload)} bytes from {url}")
                      return payload
              except urllib.error.HTTPError as exc:
                  print(f"Failed to fetch {url}: {exc}")
                  return ""

          def collect_meeting_files(base_url):
              queue = [base_url]
              seen = set()
              files = []
              print("Collecting meeting files by traversing directories under base URL.")
              while queue:
                  url = queue.pop(0)
                  if url in seen:
                      continue
                  seen.add(url)
                  html = fetch_url(url)
                  if not html:
                      print(f"Skipping {url} due to empty response.")
                      continue
                  parser = LinkParser()
                  parser.feed(html)
                  for link in parser.links:
                      if link.startswith("#"):
                          continue
                      next_url = urllib.parse.urljoin(url, link)
                      if not next_url.startswith(base_url):
                          print(f"Skipping external link: {next_url}")
                          continue
                      if link.endswith("/"):
                          print(f"Discovered directory: {next_url}")
                          queue.append(next_url)
                          continue
                      relative_path = urllib.parse.urlparse(next_url).path.replace(
                          urllib.parse.urlparse(base_url).path,
                          "",
                          1,
                      ).lstrip("/")
                      print(f"Discovered file: {relative_path}")
                      files.append(relative_path)
              print(f"Traversal complete. Found {len(files)} file(s).")
              return files

          def github_graphql(token, query, variables):
              payload = json.dumps({"query": query, "variables": variables}).encode("utf-8")
              request = urllib.request.Request(
                  "https://api.github.com/graphql",
                  data=payload,
                  headers={
                      "Authorization": f"Bearer {token}",
                      "Content-Type": "application/json",
                  },
              )
              with urllib.request.urlopen(request) as response:
                  data = json.loads(response.read().decode("utf-8"))
              if "errors" in data:
                  raise RuntimeError(data["errors"])
              return data["data"]

          days_back_raw = os.getenv("DAYS_BACK") or "30"
          try:
              days_back = int(days_back_raw)
          except ValueError:
              print(f"Invalid DAYS_BACK value: {days_back_raw}")
              sys.exit(1)

          cutoff_date = datetime.date.today() - datetime.timedelta(days=days_back)
          print(f"Filtering meeting notes from the last {days_back} day(s); cutoff date: {cutoff_date}")

          valid_suffixes = (".html", ".txt", ".log", ".log.html", ".log.txt")
          required_path_fragment = "govpp_community_meeting"
          meeting_files = []
          for link in collect_meeting_files(BASE_URL):
              if required_path_fragment not in link:
                  print(f"Skipping non-govpp community meeting path: {link}")
                  continue
              if not link.lower().endswith(valid_suffixes):
                  print(f"Skipping non-meeting file (extension mismatch): {link}")
                  continue
              meeting_files.append(link)

          if not meeting_files:
              print("No meeting notes found.")
              sys.exit(0)

          def extract_date(filename):
              match = re.search(r"(20\d{2}-\d{2}-\d{2})", filename)
              if not match:
                  return None
              try:
                  return datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()
              except ValueError:
                  return None

          filtered = []
          for filename in meeting_files:
              note_date = extract_date(filename)
              if note_date is None:
                  print(f"Skipping {filename} (no date found in filename).")
                  continue
              if note_date < cutoff_date:
                  print(f"Skipping {filename} (date {note_date} older than cutoff).")
                  continue
              print(f"Including {filename} (date {note_date}).")
              filtered.append(filename)

          meeting_files = sorted(set(filtered))
          print(f"{len(meeting_files)} meeting file(s) remain after filtering and de-duplication.")
          if not meeting_files:
              print("No meeting notes within the configured date window.")
              sys.exit(0)

          token = os.environ.get("GITHUB_TOKEN")
          if not token:
              print("GITHUB_TOKEN is required.")
              sys.exit(1)

          query = """
          query($owner: String!, $repo: String!, $number: Int!, $cursor: String) {
            repository(owner: $owner, name: $repo) {
              discussion(number: $number) {
                id
                comments(first: 100, after: $cursor) {
                  nodes { body }
                  pageInfo { hasNextPage endCursor }
                }
              }
            }
          }
          """

          discussion_id = None
          comments = []
          cursor = None
          while True:
              data = github_graphql(
                  token,
                  query,
                  {
                      "owner": OWNER,
                      "repo": REPO,
                      "number": DISCUSSION_NUMBER,
                      "cursor": cursor,
                  },
              )
              discussion = data["repository"]["discussion"]
              if discussion is None:
                  print("Discussion not found.")
                  sys.exit(1)
              discussion_id = discussion_id or discussion["id"]
              comments.extend(node["body"] for node in discussion["comments"]["nodes"])
              page_info = discussion["comments"]["pageInfo"]
              if not page_info["hasNextPage"]:
                  break
              cursor = page_info["endCursor"]
          print(f"Fetched {len(comments)} existing discussion comment(s) for de-duplication.")

          existing_markers = set()
          marker_pattern = re.compile(r"<!-- meeting-notes:([^\s>]+) -->")
          for body in comments:
              for match in marker_pattern.findall(body):
                  existing_markers.add(match)
          print(f"Found {len(existing_markers)} existing meeting note marker(s).")

          mutation = """
          mutation($discussionId: ID!, $body: String!) {
            addDiscussionComment(input: {discussionId: $discussionId, body: $body}) {
              comment { id url }
            }
          }
          """

          posted = 0
          for filename in meeting_files:
              if filename in existing_markers:
                  print(f"Skipping already-posted meeting notes: {filename}")
                  continue
              note_url = urllib.parse.urljoin(BASE_URL, filename)
              note_date = extract_date(filename)
              if note_date is None:
                  print(f"Skipping {filename} during posting (no date found in filename).")
                  continue
              badge_date = note_date.strftime("%Y-%b-%d")
              badge_label = f"{badge_date}-GoVPP Community Meeting"
              badge_label_escaped = urllib.parse.quote(badge_label.replace("-", "--"))
              badge_url = f"https://img.shields.io/badge/{badge_label_escaped}-16b"
              body = (
                  f"### FD.io meeting notes `{filename}`\n\n"
                  f"<a href=\"{note_url}\"><img src=\"{badge_url}\"></a>\n\n"
                  f"Source: {note_url}\n\n"
                  f"<!-- meeting-notes:{filename} -->"
              )
              print(f"Posting new meeting notes comment for {filename}")
              result = github_graphql(
                  token,
                  mutation,
                  {"discussionId": discussion_id, "body": body},
              )
              comment_url = result["addDiscussionComment"]["comment"]["url"]
              print(f"Posted comment: {comment_url}")
              posted += 1

          print(f"Posted {posted} new meeting note comment(s).")
          PY
