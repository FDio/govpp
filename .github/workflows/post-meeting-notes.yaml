name: Post FD.io meeting notes

on:
  schedule:
    - cron: "23 3 * * *"
  workflow_dispatch:
    inputs:
      days_back:
        description: "Days to look back for meeting notes"
        required: false
        default: "30"
      year_min:
        description: "Minimum meeting year to include (empty defaults to this year and last year)"
        required: false
        default: ""
      dry_run:
        description: "When true, do not post comments; only print planned comments"
        required: false
        default: "true"

permissions:
  contents: read
  discussions: write

jobs:
  post-meeting-notes:
    runs-on: ubuntu-latest
    steps:
      - name: Post meeting notes to discussion
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          DAYS_BACK: ${{ github.event.inputs.days_back }}
          YEAR_MIN: ${{ github.event.inputs.year_min }}
          DRY_RUN: ${{ github.event.inputs.dry_run }}
          SUMMARY_PATH: meeting-notes-summary.txt
        run: |
          python - <<'PY'
          import datetime
          import json
          import os
          import re
          import sys
          import urllib.error
          import urllib.parse
          import urllib.request
          from html.parser import HTMLParser

          BASE_URL = "https://ircbot.wl.linuxfoundation.org/meetings/fdio-meeting/"
          OWNER = "FDio"
          REPO = "govpp"
          DISCUSSION_NUMBER = 46

          summary_path = os.getenv("SUMMARY_PATH", "meeting-notes-summary.txt")
          dry_run_raw = (os.getenv("DRY_RUN") or "true").strip().lower()
          dry_run = dry_run_raw in {"1", "true", "yes", "y", "on"}

          print(f"Starting meeting notes sync from {BASE_URL}")
          print(f"Target discussion: {OWNER}/{REPO}#{DISCUSSION_NUMBER}")
          print(f"Dry run enabled: {dry_run}")

          class LinkParser(HTMLParser):
              def __init__(self):
                  super().__init__()
                  self.links = []

              def handle_starttag(self, tag, attrs):
                  if tag != "a":
                      return
                  for key, value in attrs:
                      if key == "href" and value:
                          self.links.append(value)

          def fetch_url(url):
              print(f"Fetching URL: {url}")
              try:
                  with urllib.request.urlopen(url) as response:
                      payload = response.read().decode("utf-8", errors="replace")
                      print(f"Fetched {len(payload)} bytes from {url}")
                      return payload
              except urllib.error.HTTPError as exc:
                  print(f"Failed to fetch {url}: {exc}")
                  return ""

          def collect_meeting_files(base_url):
              queue = [base_url]
              seen = set()
              files = []
              print("Collecting meeting files by traversing directories under base URL.")
              while queue:
                  url = queue.pop(0)
                  if url in seen:
                      continue
                  seen.add(url)
                  html = fetch_url(url)
                  if not html:
                      print(f"Skipping {url} due to empty response.")
                      continue
                  parser = LinkParser()
                  parser.feed(html)
                  for link in parser.links:
                      if link.startswith("#"):
                          continue
                      next_url = urllib.parse.urljoin(url, link)
                      if not next_url.startswith(base_url):
                          print(f"Skipping external link: {next_url}")
                          continue
                      if link.endswith("/"):
                          print(f"Discovered directory: {next_url}")
                          queue.append(next_url)
                          continue
                      relative_path = urllib.parse.urlparse(next_url).path.replace(
                          urllib.parse.urlparse(base_url).path,
                          "",
                          1,
                      ).lstrip("/")
                      print(f"Discovered file: {relative_path}")
                      files.append(relative_path)
              print(f"Traversal complete. Found {len(files)} file(s).")
              return files

          def github_graphql(token, query, variables):
              payload = json.dumps({"query": query, "variables": variables}).encode("utf-8")
              request = urllib.request.Request(
                  "https://api.github.com/graphql",
                  data=payload,
                  headers={
                      "Authorization": f"Bearer {token}",
                      "Content-Type": "application/json",
                  },
              )
              with urllib.request.urlopen(request) as response:
                  data = json.loads(response.read().decode("utf-8"))
              if "errors" in data:
                  raise RuntimeError(data["errors"])
              return data["data"]

          days_back_raw = os.getenv("DAYS_BACK") or "30"
          try:
              days_back = int(days_back_raw)
          except ValueError:
              print(f"Invalid DAYS_BACK value: {days_back_raw}")
              sys.exit(1)

          cutoff_date = datetime.date.today() - datetime.timedelta(days=days_back)
          current_year = datetime.date.today().year
          year_min_raw = (os.getenv("YEAR_MIN") or "").strip()
          if year_min_raw:
              try:
                  year_min = int(year_min_raw)
              except ValueError:
                  print(f"Invalid YEAR_MIN value: {year_min_raw}")
                  sys.exit(1)
              allowed_years = None
          else:
              year_min = current_year - 1
              allowed_years = {current_year - 1, current_year}

          print(f"Filtering meeting notes from the last {days_back} day(s); cutoff date: {cutoff_date}")
          if allowed_years is None:
              print(f"Filtering meeting notes with year >= {year_min}")
          else:
              print(f"Filtering meeting notes to years: {sorted(allowed_years)}")

          valid_suffixes = (".html", ".txt", ".log", ".log.html", ".log.txt")
          required_path_fragment = "govpp_community_meeting"
          stats = {
              "total_discovered": 0,
              "path_filtered": 0,
              "extension_filtered": 0,
              "missing_date": 0,
              "too_old": 0,
              "too_old_year": 0,
              "eligible": 0,
              "existing_markers": 0,
              "planned": 0,
              "posted": 0,
          }
          meeting_files = []
          for link in collect_meeting_files(BASE_URL):
              stats["total_discovered"] += 1
              if required_path_fragment not in link:
                  stats["path_filtered"] += 1
                  print(f"Skipping non-govpp community meeting path: {link}")
                  continue
              if not link.lower().endswith(valid_suffixes):
                  stats["extension_filtered"] += 1
                  print(f"Skipping non-meeting file (extension mismatch): {link}")
                  continue
              meeting_files.append(link)

          if not meeting_files:
              print("No meeting notes found.")
              sys.exit(0)

          def extract_date(filename):
              match = re.search(r"(20\d{2}-\d{2}-\d{2})", filename)
              if not match:
                  return None
              try:
                  return datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()
              except ValueError:
                  return None

          filtered = []
          for filename in meeting_files:
              note_date = extract_date(filename)
              if note_date is None:
                  stats["missing_date"] += 1
                  print(f"Skipping {filename} (no date found in filename).")
                  continue
              if allowed_years is not None and note_date.year not in allowed_years:
                  stats["too_old_year"] += 1
                  print(f"Skipping {filename} (year {note_date.year} outside allowed years).")
                  continue
              if allowed_years is None and note_date.year < year_min:
                  stats["too_old_year"] += 1
                  print(f"Skipping {filename} (year {note_date.year} older than minimum {year_min}).")
                  continue
              if note_date < cutoff_date:
                  stats["too_old"] += 1
                  print(f"Skipping {filename} (date {note_date} older than cutoff).")
                  continue
              print(f"Including {filename} (date {note_date}).")
              filtered.append(filename)

          meeting_files = sorted(set(filtered))
          stats["eligible"] = len(meeting_files)
          print(f"{len(meeting_files)} meeting file(s) remain after filtering and de-duplication.")
          if not meeting_files:
              print("No meeting notes within the configured date window.")
              sys.exit(0)

          token = os.environ.get("GITHUB_TOKEN")
          if not token:
              print("GITHUB_TOKEN is required.")
              sys.exit(1)

          query = """
          query($owner: String!, $repo: String!, $number: Int!, $cursor: String) {
            repository(owner: $owner, name: $repo) {
              discussion(number: $number) {
                id
                comments(first: 100, after: $cursor) {
                  nodes { body }
                  pageInfo { hasNextPage endCursor }
                }
              }
            }
          }
          """

          discussion_id = None
          comments = []
          cursor = None
          while True:
              data = github_graphql(
                  token,
                  query,
                  {
                      "owner": OWNER,
                      "repo": REPO,
                      "number": DISCUSSION_NUMBER,
                      "cursor": cursor,
                  },
              )
              discussion = data["repository"]["discussion"]
              if discussion is None:
                  print("Discussion not found.")
                  sys.exit(1)
              discussion_id = discussion_id or discussion["id"]
              comments.extend(node["body"] for node in discussion["comments"]["nodes"])
              page_info = discussion["comments"]["pageInfo"]
              if not page_info["hasNextPage"]:
                  break
              cursor = page_info["endCursor"]
          print(f"Fetched {len(comments)} existing discussion comment(s) for de-duplication.")

          existing_markers = set()
          marker_pattern = re.compile(r"<!-- meeting-notes:([^\s>]+) -->")
          for body in comments:
              for match in marker_pattern.findall(body):
                  existing_markers.add(match)
          stats["existing_markers"] = len(existing_markers)
          print(f"Found {len(existing_markers)} existing meeting note marker(s).")

          mutation = """
          mutation($discussionId: ID!, $body: String!) {
            addDiscussionComment(input: {discussionId: $discussionId, body: $body}) {
              comment { id url }
            }
          }
          """

          planned_bodies = []
          posted = 0
          for filename in meeting_files:
              if filename in existing_markers:
                  print(f"Skipping already-posted meeting notes: {filename}")
                  continue
              note_url = urllib.parse.urljoin(BASE_URL, filename)
              note_date = extract_date(filename)
              if note_date is None:
                  print(f"Skipping {filename} during posting (no date found in filename).")
                  continue
              badge_date = note_date.strftime("%Y-%b-%d").replace("-", "--")
              badge_label = f"{badge_date}-GoVPP Community Meeting"
              badge_label_escaped = urllib.parse.quote(badge_label)
              badge_url = f"https://img.shields.io/badge/{badge_label_escaped}-16b"
              body = (
                  "## FDio-GoVPP Community Meeting - Notes\n\n"
                  f"<a href=\"{note_url}\"><img src=\"{badge_url}\"></a>\n\n"
                  f"<sub>Source: [#fdio-meeting: GoVPP Community Meeting]({note_url})</sub>\n\n"
                  f"<!-- meeting-notes:{filename} -->"
              )
              planned_bodies.append(body)
              stats["planned"] += 1
              print(f"Posting new meeting notes comment for {filename}")
              if dry_run:
                  print("Dry run: skipping comment creation.")
                  print(body)
              else:
                  result = github_graphql(
                      token,
                      mutation,
                      {"discussionId": discussion_id, "body": body},
                  )
                  comment_url = result["addDiscussionComment"]["comment"]["url"]
                  print(f"Posted comment: {comment_url}")
                  posted += 1

          stats["posted"] = posted
          print(f"Posted {posted} new meeting note comment(s).")

          with open(summary_path, "w", encoding="utf-8") as summary_file:
              summary_file.write("Meeting notes sync summary\n")
              summary_file.write("==========================\n")
              summary_file.write(f"Base URL: {BASE_URL}\n")
              summary_file.write(f"Target discussion: {OWNER}/{REPO}#{DISCUSSION_NUMBER}\n")
              summary_file.write(f"Dry run: {dry_run}\n")
              summary_file.write(f"Cutoff date: {cutoff_date}\n")
              summary_file.write(f"Total discovered: {stats['total_discovered']}\n")
              summary_file.write(f"Filtered (path fragment): {stats['path_filtered']}\n")
              summary_file.write(f"Filtered (extension): {stats['extension_filtered']}\n")
              summary_file.write(f"Filtered (missing date): {stats['missing_date']}\n")
              summary_file.write(f"Filtered (too old): {stats['too_old']}\n")
              summary_file.write(f"Filtered (year constraint): {stats['too_old_year']}\n")
              summary_file.write(f"Eligible after filtering: {stats['eligible']}\n")
              summary_file.write(f"Existing markers: {stats['existing_markers']}\n")
              summary_file.write(f"Planned comments: {stats['planned']}\n")
              summary_file.write(f"Posted comments: {stats['posted']}\n")
              summary_file.write("\nPlanned comment bodies:\n")
              summary_file.write("-----------------------\n")
              if not planned_bodies:
                  summary_file.write("None\n")
              else:
                  for body in planned_bodies:
                      summary_file.write(body)
                      summary_file.write("\n\n---\n\n")
          PY
      - name: Summarize meeting notes run
        run: |
          echo "Meeting notes workflow summary"
          echo "-------------------------------"
          cat meeting-notes-summary.txt
